# Google Sheets API í†µí•© ê¸°ìˆ  ê²€í†  ë¦¬í¬íŠ¸

## ë¬¸ì„œ ê°œìš”
- **ì‘ì„±ì¼**: 2025-01-18
- **ëª©ì **: í•´ì–‘ë¶€ì²˜ í¬ë¡¤ëŸ¬ ëŒ€ì‹œë³´ë“œì˜ Google Sheets API í†µí•© ê²€ì¦
- **ê²€í†  ëŒ€ìƒ**: Python í¬ë¡¤ëŸ¬ (upload_to_gsheet.py) + Next.js ëŒ€ì‹œë³´ë“œ

---

## ğŸ“‹ ëª©ì°¨
1. [ìš”ì•½ (Executive Summary)](#ìš”ì•½-executive-summary)
2. [Python í¬ë¡¤ëŸ¬ ì¸¡ ê²€í† ](#python-í¬ë¡¤ëŸ¬-ì¸¡-ê²€í† )
3. [Next.js í´ë¼ì´ì–¸íŠ¸ ì¸¡ ê²€í† ](#nextjs-í´ë¼ì´ì–¸íŠ¸-ì¸¡-ê²€í† )
4. [ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë° ì¿¼ë¦¬](#ë°ì´í„°-ìŠ¤í‚¤ë§ˆ-ë°-ì¿¼ë¦¬)
5. [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
6. [ëŒ€ì•ˆ ê²€í† ](#ëŒ€ì•ˆ-ê²€í† )
7. [ê¶Œì¥ì‚¬í•­ ë° ê²°ë¡ ](#ê¶Œì¥ì‚¬í•­-ë°-ê²°ë¡ )

---

## ìš”ì•½ (Executive Summary)

### âœ… ê¸ì • í‰ê°€
- Python í¬ë¡¤ëŸ¬ì˜ Google Sheets í†µí•©ì€ **í”„ë¡œë•ì…˜ ë ˆë””(Production-Ready)** ìˆ˜ì¤€
- gspread ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ì ì ˆ, Service Account ì¸ì¦ ì•ˆì •ì 
- ì¤‘ë³µ ì œê±°, ë°°ì¹˜ ì—…ë¡œë“œ, ì—´ ë„ˆë¹„ ìë™ ì¡°ì • ë“± ì‹¤ìš©ì  ê¸°ëŠ¥ êµ¬í˜„

### âš ï¸ ì£¼ìš” ë¦¬ìŠ¤í¬
1. **Google Sheets API ì¿¼í„° ì œí•œ** (ì½ê¸°: ë¶„ë‹¹ 100íšŒ, ì“°ê¸°: ë¶„ë‹¹ 60íšŒ)
2. **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í•œê³„** (1,000ê±´ ì´ìƒ ì‹œ ì„±ëŠ¥ ì €í•˜)
3. **Next.js í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‹¤ì‹œê°„ ë™ê¸°í™” ë³µì¡ì„±**
4. **ë‹¤ìˆ˜ ì‚¬ìš©ì ë™ì‹œ ì ‘ì† ì‹œ API Rate Limit ë„ë‹¬ ê°€ëŠ¥**

### ğŸ’¡ í•µì‹¬ ê¶Œì¥ì‚¬í•­
- **ë‹¨ê¸°**: Google Sheets ìœ ì§€ + ì½ê¸° ì¿¼í„° ìµœì í™” (ISR ìºì‹± 5ë¶„)
- **ì¤‘ê¸°**: Supabase PostgreSQLë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ (ë¬´ë£Œ í”Œëœ 500MB, 2GB ì „ì†¡)
- **ì¥ê¸°**: Vercel Postgres + Edge Functions (ê¸€ë¡œë²Œ í™•ì¥ì„±)

---

## Python í¬ë¡¤ëŸ¬ ì¸¡ ê²€í† 

### 1. gspread ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ì˜ ì ì ˆì„±

#### âœ… **ì ì ˆí•œ ì„ íƒ**

**gspreadì˜ ì¥ì **:
```python
# ê°„ê²°í•œ API
worksheet.append_rows(values)  # ë°°ì¹˜ ì—…ë¡œë“œ
worksheet.get_all_values()     # ì „ì²´ ë°ì´í„° ì½ê¸°
```

**ëŒ€ì•ˆ ë¹„êµ**:

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì¥ì  | ë‹¨ì  | í‰ê°€ |
|-----------|------|------|------|
| **gspread** | Pythonic API, ë°°ì¹˜ ì§€ì› | ì˜ì¡´ì„± ì¶”ê°€ (google-auth) | â­â­â­â­â­ |
| googleapis (Python) | ê³µì‹ ì§€ì›, ì„¸ë°€í•œ ì œì–´ | Verboseí•œ ì½”ë“œ | â­â­â­ |
| pygsheets | DataFrame ì§ì ‘ ì§€ì› | ìœ ì§€ë³´ìˆ˜ ëŠë¦¼ | â­â­â­ |

**íŒì •**: **gspread ì‚¬ìš© ìœ ì§€ ê¶Œì¥**

---

### 2. Service Account ì¸ì¦ ë°©ì‹

#### âœ… **ìµœì ì˜ ì„ íƒ**

**í˜„ì¬ êµ¬í˜„**:
```python
scopes = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]
credentials = Credentials.from_service_account_file(
    self.credentials_file,
    scopes=scopes
)
```

**ì¥ì **:
- âœ… ì‚¬ìš©ì ì¸ì¦ ë¶ˆí•„ìš” (ìë™í™”ì— í•„ìˆ˜)
- âœ… í† í° ë§Œë£Œ ìë™ ê°±ì‹ 
- âœ… ê³µìœ  ì‹œíŠ¸ í¸ì§‘ ê¶Œí•œ ì œì–´ ìš©ì´

**ë³´ì•ˆ ê³ ë ¤ì‚¬í•­**:
```python
# âŒ ì ˆëŒ€ ê¸ˆì§€
CREDENTIALS_FILE = r'C:\AI\251118\gen-lang-client-0556505482-e847371ea87e.json'

# âœ… í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥
import os
CREDENTIALS_FILE = os.getenv('GOOGLE_CREDENTIALS_PATH')

# âœ… ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ JSON ë¡œë“œ
import json
credentials_json = os.getenv('GOOGLE_CREDENTIALS_JSON')
credentials = Credentials.from_service_account_info(
    json.loads(credentials_json),
    scopes=scopes
)
```

**íŒì •**: **êµ¬í˜„ ìš°ìˆ˜, ë³´ì•ˆ ê°œì„  í•„ìš”**

---

### 3. append_rows() ë°°ì¹˜ ì—…ë¡œë“œ ì„±ëŠ¥

#### âœ… **ì„±ëŠ¥ ìµœì í™”ë¨**

**í˜„ì¬ êµ¬í˜„**:
```python
values = new_df.values.tolist()
worksheet.append_rows(values)  # ë°°ì¹˜ë¡œ ì¶”ê°€
```

**ì„±ëŠ¥ ë¹„êµ**:

| ë°©ì‹ | 100ê±´ ì—…ë¡œë“œ ì‹œê°„ | API í˜¸ì¶œ íšŸìˆ˜ | í‰ê°€ |
|------|------------------|--------------|------|
| **append_rows()** | ~1-2ì´ˆ | 1íšŒ | â­â­â­â­â­ |
| append_row() ë°˜ë³µ | ~30-60ì´ˆ | 100íšŒ | â­ |
| batch_update() | ~1ì´ˆ | 1íšŒ | â­â­â­â­â­ |

**ì¶”ê°€ ìµœì í™” ê°€ëŠ¥**:
```python
# í˜„ì¬ ë°©ì‹
worksheet.append_rows(values)

# ë” ë¹ ë¥¸ ë°©ì‹ (ëŒ€ìš©ëŸ‰ ì‹œ)
worksheet.batch_update([{
    'range': f'A{start_row}',
    'values': values
}], value_input_option='USER_ENTERED')
```

**íŒì •**: **í˜„ì¬ êµ¬í˜„ ì¶©ë¶„, 1000ê±´ ì´ìƒ ì‹œ batch_update ê²€í† **

---

### 4. ì¤‘ë³µ ì œê±° ë¡œì§ (ë§í¬ ê¸°ì¤€)

#### âœ… **íš¨ê³¼ì ì´ë‚˜ ê°œì„  ê°€ëŠ¥**

**í˜„ì¬ êµ¬í˜„**:
```python
if not existing_df.empty and 'ë§í¬' in existing_df.columns:
    existing_links = set(existing_df['ë§í¬'].tolist())
    new_df = df[~df['ë§í¬'].isin(existing_links)].copy()
    duplicate_count = len(df) - len(new_df)
```

**ë¬¸ì œì **:
1. **ì „ì²´ ë°ì´í„° ë¡œë“œ** â†’ 1000ê±´ ì´ìƒ ì‹œ ë©”ëª¨ë¦¬/ì‹œê°„ ì†Œëª¨
2. **ëŒ€ì†Œë¬¸ì êµ¬ë¶„** â†’ `example.com/ABC`ì™€ `example.com/abc` ì¤‘ë³µ ë¯¸ê°ì§€
3. **ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ ì°¨ì´** â†’ `url?page=1`ê³¼ `url?page=2` ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ íŒë‹¨

**ê°œì„ ì•ˆ**:
```python
# 1. URL ì •ê·œí™” ì¶”ê°€
from urllib.parse import urlparse, parse_qs, urlencode

def normalize_url(url):
    """URL ì •ê·œí™” (ëŒ€ì†Œë¬¸ì, ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬)"""
    parsed = urlparse(url.lower())
    # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬
    query = parse_qs(parsed.query)
    sorted_query = urlencode(sorted(query.items()), doseq=True)
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{sorted_query}"

df['ë§í¬_ì •ê·œí™”'] = df['ë§í¬'].apply(normalize_url)
new_df = df[~df['ë§í¬_ì •ê·œí™”'].isin(existing_links)].copy()

# 2. ë¶€ë¶„ ë°ì´í„°ë§Œ ë¡œë“œ (ìµœê·¼ Nì¼)
# Google Sheetsì—ì„œ ì „ì²´ ë¡œë“œ ëŒ€ì‹  ìµœê·¼ 30ì¼ë§Œ í™•ì¸
# (í•˜ì§€ë§Œ Google SheetsëŠ” SQL ì¿¼ë¦¬ ë¯¸ì§€ì› â†’ ì „ì²´ ë¡œë“œ ë¶ˆê°€í”¼)
```

**íŒì •**: **ê¸°ë³¸ ë¡œì§ ìš°ìˆ˜, URL ì •ê·œí™” ì¶”ê°€ ê¶Œì¥**

---

### 5. ì—´ ë„ˆë¹„ ìë™ ì¡°ì • êµ¬í˜„

#### âœ… **ë›°ì–´ë‚œ UX ê°œì„ **

**í˜„ì¬ êµ¬í˜„**:
```python
# í•œê¸€ 2ê¸€ì, ì˜ë¬¸ 1ê¸€ìë¡œ ê³„ì‚°
cell_length = sum(2 if ord(c) > 127 else 1 for c in str(row[col_idx]))
width = min(max(max_length * 7, 100), 600)  # ìµœì†Œ 100px, ìµœëŒ€ 600px
```

**ì¥ì **:
- âœ… í•œê¸€/ì˜ë¬¸ ë„ˆë¹„ ì°¨ì´ ê³ ë ¤
- âœ… ìµœì†Œ/ìµœëŒ€ ë„ˆë¹„ ì œí•œìœ¼ë¡œ UI ì•ˆì •ì„± í™•ë³´
- âœ… batch_updateë¡œ í•œ ë²ˆì— ì²˜ë¦¬ (API íš¨ìœ¨ì )

**ê°œì„  ê°€ëŠ¥ì **:
```python
# 1. ì…€ ë‚´ìš© ì¼ë¶€ë§Œ ìƒ˜í”Œë§ (ëŒ€ìš©ëŸ‰ ì‹œ ì†ë„ ê°œì„ )
sampled_rows = all_values[:100]  # ì²˜ìŒ 100í–‰ë§Œ ìƒ˜í”Œë§

# 2. ë‚ ì§œ/ìˆ«ì ì»¬ëŸ¼ ê³ ì • ë„ˆë¹„ ì„¤ì •
COLUMN_WIDTHS = {
    'ì‘ì„±ì¼': 120,
    'ìˆ˜ì§‘ì¼ì‹œ': 150,
    'ì œëª©': 400,
    'ë§í¬': 250
}
```

**íŒì •**: **í˜„ì¬ êµ¬í˜„ ìš°ìˆ˜, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œ ìƒ˜í”Œë§ ê²€í† **

---

### 6. Python í¬ë¡¤ëŸ¬ ì¢…í•© í‰ê°€

| í•­ëª© | í‰ê°€ | ê°œì„ ì‚¬í•­ |
|------|------|---------|
| ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ | â­â­â­â­â­ | - |
| ì¸ì¦ ë°©ì‹ | â­â­â­â­ | í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© |
| ë°°ì¹˜ ì—…ë¡œë“œ | â­â­â­â­â­ | - |
| ì¤‘ë³µ ì œê±° | â­â­â­â­ | URL ì •ê·œí™” |
| ì—´ ë„ˆë¹„ ì¡°ì • | â­â­â­â­â­ | - |
| ì—ëŸ¬ ì²˜ë¦¬ | â­â­â­ | ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ |

**ì¢…í•©**: **ğŸŸ¢ í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥ (ë§ˆì´ë„ˆ ê°œì„  ê¶Œì¥)**

---

## Next.js í´ë¼ì´ì–¸íŠ¸ ì¸¡ ê²€í† 

### 1. googleapis ë¼ì´ë¸ŒëŸ¬ë¦¬ vs gspread ì„ íƒ

#### âœ… **Next.jsì—ì„œëŠ” googleapis ê¶Œì¥**

**ì´ìœ **:

| ìš”ì¸ | googleapis | gspread (Python) | íŒì • |
|------|-----------|------------------|------|
| JavaScript/TypeScript ì§€ì› | ë„¤ì´í‹°ë¸Œ | ì—†ìŒ | âœ… googleapis |
| Next.js API Routes í†µí•© | ìš°ìˆ˜ | ë¶ˆê°€ëŠ¥ | âœ… googleapis |
| íƒ€ì… ì•ˆì •ì„± | TypeScript íƒ€ì… ì œê³µ | N/A | âœ… googleapis |
| ê³µì‹ ì§€ì› | Google ê³µì‹ | ì»¤ë®¤ë‹ˆí‹° | âœ… googleapis |
| ë²ˆë“¤ í¬ê¸° | ~50KB | N/A | âœ… googleapis |

**ê¶Œì¥ êµ¬í˜„**:
```typescript
// lib/googleSheets.ts
import { google } from 'googleapis';
import { JWT } from 'google-auth-library';

const SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly'];
const SPREADSHEET_ID = process.env.GOOGLE_SPREADSHEET_ID!;

// Service Account ì¸ì¦
const auth = new JWT({
  email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,
  key: process.env.GOOGLE_PRIVATE_KEY?.replace(/\\n/g, '\n'),
  scopes: SCOPES,
});

const sheets = google.sheets({ version: 'v4', auth });

export async function getSheetData(range: string = 'í¬ë¡¤ë§ ê²°ê³¼!A:G') {
  try {
    const response = await sheets.spreadsheets.values.get({
      spreadsheetId: SPREADSHEET_ID,
      range,
    });

    return response.data.values || [];
  } catch (error) {
    console.error('Google Sheets API Error:', error);
    throw error;
  }
}
```

**íŒì •**: **googleapis ì‚¬ìš© í•„ìˆ˜**

---

### 2. ISR 5ë¶„ ìºì‹± ì „ëµ íƒ€ë‹¹ì„±

#### âš ï¸ **ì ì ˆí•˜ë‚˜ ì¡°ê±´ë¶€ ê¶Œì¥**

**í˜„ì¬ ì œì•ˆ (ì¶”ì •)**:
```typescript
// app/dashboard/page.tsx
export const revalidate = 300; // 5ë¶„ ISR

export default async function DashboardPage() {
  const data = await getSheetData();
  // ...
}
```

**ë¶„ì„**:

| ì‹œë‚˜ë¦¬ì˜¤ | 5ë¶„ ìºì‹± ì í•©ì„± | ì´ìœ  |
|---------|---------------|------|
| ì‚¬ìš©ì 10ëª… ë¯¸ë§Œ | âœ… ì í•© | API ì¿¼í„° ì ˆì•½ |
| ì‚¬ìš©ì 100ëª… ì´ìƒ | âœ… ë§¤ìš° ì í•© | ì½ê¸° 100íšŒ/ë¶„ ì œí•œ íšŒí”¼ |
| ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ í•„ìš” | âŒ ë¶€ì í•© | ìµœëŒ€ 5ë¶„ ì§€ì—° |
| í¬ë¡¤ëŸ¬ 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ | âœ… ì í•© | ì¶©ë¶„íˆ ë¹ ë¥¸ ë°˜ì˜ |

**ëŒ€ì•ˆ ì „ëµ**:

```typescript
// 1. On-Demand Revalidation (ì‚¬ìš©ì ìš”ì²­ ì‹œ ê°±ì‹ )
// app/api/revalidate/route.ts
import { revalidatePath } from 'next/cache';

export async function POST(request: Request) {
  const secret = request.headers.get('x-revalidate-secret');

  if (secret !== process.env.REVALIDATE_SECRET) {
    return Response.json({ error: 'Invalid secret' }, { status: 401 });
  }

  revalidatePath('/dashboard');
  return Response.json({ revalidated: true });
}

// Python í¬ë¡¤ëŸ¬ì—ì„œ í˜¸ì¶œ
import requests

def trigger_nextjs_revalidation():
    requests.post(
        'https://yourdomain.com/api/revalidate',
        headers={'x-revalidate-secret': os.getenv('REVALIDATE_SECRET')}
    )

# í¬ë¡¤ë§ ì™„ë£Œ í›„
uploader.upload_data(df)
trigger_nextjs_revalidation()  # Next.js ìºì‹œ ì¦‰ì‹œ ê°±ì‹ 
```

```typescript
// 2. Client-side SWR (5ë¶„ ìºì‹œ + ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹ )
'use client';
import useSWR from 'swr';

export function DashboardClient() {
  const { data, error } = useSWR('/api/listings', fetcher, {
    refreshInterval: 60000,  // 1ë¶„ë§ˆë‹¤ ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹ 
    revalidateOnFocus: true, // íƒ­ ë³µê·€ ì‹œ ê°±ì‹ 
  });

  // ...
}
```

**íŒì •**: **5ë¶„ ISR + On-Demand Revalidation ì¡°í•© ê¶Œì¥**

---

### 3. ì½ê¸° ì „ìš© ê¶Œí•œ ë¶„ë¦¬ í•„ìš”ì„±

#### âœ… **ë³´ì•ˆ ê´€ì ì—ì„œ ê°•ë ¥ ê¶Œì¥**

**í˜„ì¬ ìƒíƒœ**:
- Python í¬ë¡¤ëŸ¬: `spreadsheets` + `drive` ê¶Œí•œ (ì½ê¸°/ì“°ê¸°)
- Next.js: ê¶Œí•œ ë¯¸ì •

**ê¶Œì¥ êµ¬ì¡°**:

```
Service Account 1 (Python í¬ë¡¤ëŸ¬)
  - ì´ë©”ì¼: crawler@project.iam.gserviceaccount.com
  - ê¶Œí•œ: spreadsheets (ì½ê¸°/ì“°ê¸°), drive.file
  - ìš©ë„: ë°ì´í„° ì—…ë¡œë“œ

Service Account 2 (Next.js ëŒ€ì‹œë³´ë“œ)
  - ì´ë©”ì¼: dashboard@project.iam.gserviceaccount.com
  - ê¶Œí•œ: spreadsheets.readonly
  - ìš©ë„: ë°ì´í„° ì½ê¸° ì „ìš©
```

**êµ¬í˜„**:
```typescript
// Next.js í™˜ê²½ë³€ìˆ˜
GOOGLE_SERVICE_ACCOUNT_EMAIL_READONLY=dashboard@...
GOOGLE_PRIVATE_KEY_READONLY=...

// lib/googleSheets.ts
const SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly'];

const auth = new JWT({
  email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL_READONLY,
  key: process.env.GOOGLE_PRIVATE_KEY_READONLY?.replace(/\\n/g, '\n'),
  scopes: SCOPES,
});
```

**ì´ì **:
1. **ìµœì†Œ ê¶Œí•œ ì›ì¹™** (Principle of Least Privilege)
2. **API í‚¤ íƒˆì·¨ ì‹œ í”¼í•´ ìµœì†Œí™”** (ì½ê¸°ë§Œ ê°€ëŠ¥)
3. **ê°ì‚¬ ì¶”ì  ìš©ì´** (ì–´ëŠ ì„œë¹„ìŠ¤ê°€ ì“°ê¸°í–ˆëŠ”ì§€ ëª…í™•)

**íŒì •**: **ë³„ë„ Service Account ìƒì„± ê°•ë ¥ ê¶Œì¥**

---

### 4. API ë¼ìš°íŠ¸ vs Server Components ì„ íƒ

#### âœ… **Server Components ìš°ì„ , API ë¼ìš°íŠ¸ ë³‘í–‰**

**ë¹„êµ**:

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|------|------|------|-------------|
| **Server Components** | SEO ìµœì í™”, ë¹ ë¥¸ ì´ˆê¸° ë¡œë“œ | í´ë¼ì´ì–¸íŠ¸ ì¸í„°ë™ì…˜ ì œí•œ | ëŒ€ì‹œë³´ë“œ ì´ˆê¸° ë Œë”ë§ |
| **API Routes** | ìœ ì—°í•œ ì—ëŸ¬ ì²˜ë¦¬, í´ë¼ì´ì–¸íŠ¸ í´ë§ | ì¶”ê°€ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ | ê²€ìƒ‰/í•„í„°ë§ |

**ê¶Œì¥ ì•„í‚¤í…ì²˜**:

```typescript
// 1. Server Component (ì´ˆê¸° ë°ì´í„° ë¡œë“œ)
// app/dashboard/page.tsx
import { getSheetData } from '@/lib/googleSheets';

export const revalidate = 300;

export default async function DashboardPage() {
  const data = await getSheetData();

  return (
    <div>
      <StatCards data={data} />
      <RecentPostsTable initialData={data.slice(0, 10)} />

      {/* í´ë¼ì´ì–¸íŠ¸ ì»´í¬ë„ŒíŠ¸ì— ì´ˆê¸° ë°ì´í„° ì „ë‹¬ */}
      <PostsTableClient initialData={data} />
    </div>
  );
}

// 2. API Route (ë™ì  ì¿¼ë¦¬ìš©)
// app/api/listings/route.ts
import { NextRequest } from 'next/server';
import { getSheetData } from '@/lib/googleSheets';

export const runtime = 'edge'; // Edge Runtimeìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ
export const revalidate = 60;  // 1ë¶„ ìºì‹œ

export async function GET(request: NextRequest) {
  const searchParams = request.nextUrl.searchParams;
  const agency = searchParams.get('agency');
  const board = searchParams.get('board');
  const startDate = searchParams.get('startDate');
  const endDate = searchParams.get('endDate');

  try {
    const allData = await getSheetData();

    // í•„í„°ë§
    let filteredData = allData.slice(1); // í—¤ë” ì œì™¸

    if (agency) {
      filteredData = filteredData.filter(row => row[1] === agency);
    }

    if (board) {
      filteredData = filteredData.filter(row => row[2] === board);
    }

    if (startDate && endDate) {
      filteredData = filteredData.filter(row => {
        const rowDate = new Date(row[4]);
        return rowDate >= new Date(startDate) && rowDate <= new Date(endDate);
      });
    }

    return Response.json({
      success: true,
      data: filteredData,
      total: filteredData.length,
    });
  } catch (error) {
    return Response.json(
      { success: false, error: 'Failed to fetch data' },
      { status: 500 }
    );
  }
}

// 3. Client Component (ì¸í„°ë™í‹°ë¸Œ í•„í„°ë§)
// components/PostsTableClient.tsx
'use client';
import useSWR from 'swr';

export function PostsTableClient({ initialData }) {
  const [filters, setFilters] = useState({ agency: '', board: '' });

  const { data, error } = useSWR(
    `/api/listings?${new URLSearchParams(filters)}`,
    fetcher,
    { fallbackData: initialData }
  );

  // ...
}
```

**íŒì •**: **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ê¶Œì¥ (Server Components + API Routes)**

---

### 5. Rate Limiting êµ¬í˜„ ë°©ì•ˆ

#### âš ï¸ **í•„ìˆ˜ êµ¬í˜„ (API ì¿¼í„° ë³´í˜¸)**

**Google Sheets API ì œí•œ**:
- **ì½ê¸°**: 100 requests/minute/user
- **ì“°ê¸°**: 60 requests/minute/user
- **ì¼ì¼ ë¬´ì œí•œ** (í”„ë¡œì íŠ¸ë‹¹)

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**:
```
ì‚¬ìš©ì 10ëª…ì´ ë™ì‹œì— ëŒ€ì‹œë³´ë“œ ì ‘ì†
â†’ ê°ì í•„í„°ë§ 3ë²ˆ ë³€ê²½
â†’ 10 x 3 = 30 requests/minute
â†’ ISR ìºì‹± ì—†ìœ¼ë©´ ì¿¼í„° ì´ˆê³¼ ìœ„í—˜
```

**í•´ê²° ë°©ì•ˆ**:

```typescript
// 1. Vercel Edge Config (ê¶Œì¥)
// vercel.json
{
  "crons": [
    {
      "path": "/api/cron/update-cache",
      "schedule": "*/5 * * * *"  // 5ë¶„ë§ˆë‹¤
    }
  ]
}

// app/api/cron/update-cache/route.ts
import { put } from '@vercel/edge-config';
import { getSheetData } from '@/lib/googleSheets';

export async function GET(request: Request) {
  const authHeader = request.headers.get('authorization');
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return new Response('Unauthorized', { status: 401 });
  }

  const data = await getSheetData();
  await put('sheet-data', data);

  return Response.json({ updated: true, count: data.length });
}

// app/api/listings/route.ts
import { get } from '@vercel/edge-config';

export async function GET() {
  const cachedData = await get('sheet-data');

  return Response.json({
    success: true,
    data: cachedData,
    cached: true,
  });
}

// 2. Redis ìºì‹± (ìì²´ í˜¸ìŠ¤íŒ… ì‹œ)
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

export async function GET() {
  const cached = await redis.get('sheet-data');

  if (cached) {
    return Response.json(JSON.parse(cached));
  }

  const data = await getSheetData();
  await redis.setex('sheet-data', 300, JSON.stringify(data)); // 5ë¶„ ìºì‹œ

  return Response.json(data);
}

// 3. í´ë¼ì´ì–¸íŠ¸ ì¸¡ ë””ë°”ìš´ì‹±
'use client';
import { useDebouncedCallback } from 'use-debounce';

export function SearchInput() {
  const debouncedSearch = useDebouncedCallback(
    (value) => {
      fetchFilteredData(value);
    },
    500  // 500ms ëŒ€ê¸°
  );

  return <input onChange={(e) => debouncedSearch(e.target.value)} />;
}
```

**íŒì •**: **Edge Config ë˜ëŠ” ISR ìºì‹± í•„ìˆ˜**

---

### 6. Next.js í´ë¼ì´ì–¸íŠ¸ ì¢…í•© í‰ê°€

| í•­ëª© | í‰ê°€ | ê¶Œì¥ì‚¬í•­ |
|------|------|---------|
| ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ | â­â­â­â­â­ | googleapis ì‚¬ìš© |
| ISR ìºì‹± ì „ëµ | â­â­â­â­ | 5ë¶„ + On-Demand |
| ê¶Œí•œ ë¶„ë¦¬ | â­â­â­â­â­ | ì½ê¸° ì „ìš© SA ìƒì„± |
| ì•„í‚¤í…ì²˜ | â­â­â­â­â­ | Server Components + API Routes |
| Rate Limiting | â­â­â­â­ | Edge Config ë˜ëŠ” ISR |

**ì¢…í•©**: **ğŸŸ¢ í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥ (ISR + Edge Config êµ¬í˜„ í•„ìš”)**

---

## ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë° ì¿¼ë¦¬

### 1. êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° êµ¬ì¡° ìµœì í™”

#### âš ï¸ **í˜„ì¬ ìŠ¤í‚¤ë§ˆëŠ” ë‹¨ìˆœí•˜ë‚˜ í™•ì¥ì„± ì œí•œì **

**í˜„ì¬ ìŠ¤í‚¤ë§ˆ**:
```
| ê¸°ê´€êµ¬ë¶„ | ê¸°ê´€ëª… | ê²Œì‹œíŒ | ì œëª© | ì‘ì„±ì¼ | ë§í¬ | ìˆ˜ì§‘ì¼ì‹œ |
```

**ë¶„ì„**:

| ì¸¡ë©´ | í‰ê°€ | ì´ìŠˆ |
|------|------|------|
| ì½ê¸° ì„±ëŠ¥ | â­â­â­ | ì „ì²´ ìŠ¤ìº” í•„ìš” (ì¸ë±ìŠ¤ ì—†ìŒ) |
| ì“°ê¸° ì„±ëŠ¥ | â­â­â­â­ | append_rows ë¹ ë¦„ |
| ì¿¼ë¦¬ ìœ ì—°ì„± | â­â­ | SQL ë¯¸ì§€ì› |
| ë°ì´í„° ì •í•©ì„± | â­â­ | íƒ€ì… ê°•ì œ ì—†ìŒ |
| í™•ì¥ì„± | â­â­ | 5,000í–‰ ì´ìƒ ì‹œ ëŠë¦¼ |

**ê°œì„ ì•ˆ**:

```typescript
// 1. ë³µí•© ì¸ë±ìŠ¤ ì‹œíŠ¸ ì¶”ê°€
ì›Œí¬ì‹œíŠ¸: 'ì¸ë±ìŠ¤_ê¸°ê´€ë³„'
| ê¸°ê´€ëª… | ê²Œì‹œíŒ | ìµœì‹ ì‘ì„±ì¼ | ê²Œì‹œë¬¼ìˆ˜ | ì‹œíŠ¸í–‰ë²”ìœ„ |
| ë¶€ì‚°ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­ | ê³µì§€ì‚¬í•­ | 2025-01-18 | 45 | A2:A46 |

ì›Œí¬ì‹œíŠ¸: 'ì¸ë±ìŠ¤_ë‚ ì§œë³„'
| ì‘ì„±ì¼ | ê²Œì‹œë¬¼ìˆ˜ | ì‹œíŠ¸í–‰ë²”ìœ„ |
| 2025-01-18 | 23 | A2:A24 |

// Next.jsì—ì„œ ë²”ìœ„ ì¿¼ë¦¬
const range = await getIndexRange('ë¶€ì‚°ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­', 'ê³µì§€ì‚¬í•­');
const data = await sheets.spreadsheets.values.get({
  spreadsheetId: SPREADSHEET_ID,
  range,  // 'A2:A46' â†’ ì „ì²´ ìŠ¤ìº” ëŒ€ì‹  í•„ìš”í•œ í–‰ë§Œ
});

// 2. ë°ì´í„° íƒ€ì… ëª…ì‹œ (ë°ì´í„° ê²€ì¦)
ì›Œí¬ì‹œíŠ¸: 'ìŠ¤í‚¤ë§ˆ'
| ì»¬ëŸ¼ëª… | íƒ€ì… | í•„ìˆ˜ | ê¸°ë³¸ê°’ |
| ê¸°ê´€êµ¬ë¶„ | ENUM(ì§€ë°©ì²­,ê³µë‹¨,í•­ë§Œê³µì‚¬) | Y | - |
| ì‘ì„±ì¼ | DATE(YYYY-MM-DD) | Y | - |
| ë§í¬ | URL | Y | - |

// Python í¬ë¡¤ëŸ¬ì—ì„œ ê²€ì¦
def validate_row(row, schema):
    for col, rules in schema.items():
        if rules['required'] and not row[col]:
            raise ValueError(f'{col} is required')
        if rules['type'] == 'URL' and not row[col].startswith('http'):
            raise ValueError(f'{col} must be valid URL')
```

**íŒì •**: **ë‹¨ìˆœ ì‚¬ìš©ì—ëŠ” ì¶©ë¶„, 1000ê±´ ì´ìƒ ì‹œ ì¸ë±ìŠ¤ ì‹œíŠ¸ ì¶”ê°€ ê¶Œì¥**

---

### 2. í•„í„°ë§ ì¿¼ë¦¬ ì„±ëŠ¥

#### âš ï¸ **í˜„ì¬ëŠ” í´ë¼ì´ì–¸íŠ¸ ì¸¡ í•„í„°ë§ â†’ ë¹„íš¨ìœ¨**

**ë¬¸ì œ**:
```typescript
// âŒ ì „ì²´ ë°ì´í„° ë¡œë“œ í›„ í•„í„°ë§
const allData = await getSheetData(); // 12,547ê±´
const filtered = allData.filter(row =>
  row[1] === 'ë¶€ì‚°ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­'  // 45ê±´ë§Œ í•„ìš”í•œë° 12,547ê±´ ì „ì†¡
);
```

**ê°œì„  ë°©ì•ˆ**:

```typescript
// 1. Google Sheets Query Language (ì œí•œì )
// ì°¸ê³ : Google SheetsëŠ” SQL ë¯¸ì§€ì›, Apps Scriptë¡œ ìš°íšŒ ê°€ëŠ¥

// 2. ì„œë²„ ì¸¡ í•„í„°ë§ + í˜ì´ì§€ë„¤ì´ì…˜
// app/api/listings/route.ts
export async function GET(request: NextRequest) {
  const { agency, board, page = 1, limit = 50 } = parseSearchParams(request);

  const allData = await getSheetData();

  // í•„í„°ë§
  let filtered = allData.filter(row => {
    if (agency && row[1] !== agency) return false;
    if (board && row[2] !== board) return false;
    return true;
  });

  // í˜ì´ì§€ë„¤ì´ì…˜
  const start = (page - 1) * limit;
  const end = start + limit;
  const paginated = filtered.slice(start, end);

  return Response.json({
    data: paginated,
    total: filtered.length,
    page,
    totalPages: Math.ceil(filtered.length / limit),
  });
}

// 3. ì›Œí¬ì‹œíŠ¸ ë¶„ë¦¬ (ì¶”ì²œ)
ì›Œí¬ì‹œíŠ¸: 'ë¶€ì‚°ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­_ê³µì§€ì‚¬í•­'
ì›Œí¬ì‹œíŠ¸: 'ë¶€ì‚°ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­_ì…ì°°'
ì›Œí¬ì‹œíŠ¸: 'ì¸ì²œì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­_ê³µì§€ì‚¬í•­'
...

// í•„ìš”í•œ ì‹œíŠ¸ë§Œ ë¡œë“œ
const sheetName = `${agency}_${board}`;
const data = await getSheetData(sheetName);
```

**ì„±ëŠ¥ ë¹„êµ**:

| ë°©ì‹ | 12,547ê±´ ì¤‘ 45ê±´ ì¡°íšŒ ì‹œ | API í˜¸ì¶œ | ì „ì†¡ ë°ì´í„° |
|------|-------------------------|---------|-----------|
| ì „ì²´ ë¡œë“œ + í´ë¼ì´ì–¸íŠ¸ í•„í„° | ~2ì´ˆ | 1íšŒ | 12,547ê±´ |
| ì „ì²´ ë¡œë“œ + ì„œë²„ í•„í„° | ~1.5ì´ˆ | 1íšŒ | 45ê±´ |
| ì›Œí¬ì‹œíŠ¸ ë¶„ë¦¬ | ~0.3ì´ˆ | 1íšŒ | 45ê±´ |

**íŒì •**: **ì›Œí¬ì‹œíŠ¸ ë¶„ë¦¬ ê°•ë ¥ ê¶Œì¥ (ê¸°ê´€ë³„/ê²Œì‹œíŒë³„)**

---

### 3. ëŒ€ìš©ëŸ‰ ë°ì´í„° (1000ê±´+) ì²˜ë¦¬

#### âŒ **Google SheetsëŠ” 10,000ê±´ ì´ìƒ ë¶€ì í•©**

**ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**:

| ë°ì´í„° ê±´ìˆ˜ | Google Sheets ì‘ë‹µ ì‹œê°„ | Supabase ì‘ë‹µ ì‹œê°„ | Vercel Postgres |
|-----------|----------------------|-------------------|-----------------|
| 100ê±´ | ~0.3ì´ˆ | ~0.05ì´ˆ | ~0.03ì´ˆ |
| 1,000ê±´ | ~1.2ì´ˆ | ~0.1ì´ˆ | ~0.08ì´ˆ |
| 10,000ê±´ | ~8ì´ˆ | ~0.5ì´ˆ | ~0.3ì´ˆ |
| 100,000ê±´ | âŒ íƒ€ì„ì•„ì›ƒ | ~2ì´ˆ | ~1ì´ˆ |

**Google Sheets í•œê³„**:
1. **ì…€ ì œí•œ**: ì‹œíŠ¸ë‹¹ 5,000,000ì…€ (ì˜ˆ: 7ì»¬ëŸ¼ Ã— 714,285í–‰)
2. **API ì‘ë‹µ í¬ê¸°**: 10MB ì œí•œ
3. **ì¿¼ë¦¬ ì†ë„**: ì¸ë±ìŠ¤ ì—†ì–´ì„œ O(n) ìŠ¤ìº”

**í•´ê²° ë°©ì•ˆ**:

```python
# 1. ì›”ë³„ ì‹œíŠ¸ ìë™ ë¡œí…Œì´ì…˜
def get_current_sheet_name():
    from datetime import datetime
    return f"í¬ë¡¤ë§ê²°ê³¼_{datetime.now().strftime('%Y%m')}"

# 2025ë…„ 1ì›” â†’ 'í¬ë¡¤ë§ê²°ê³¼_202501'
# 2025ë…„ 2ì›” â†’ 'í¬ë¡¤ë§ê²°ê³¼_202502' (ìƒˆ ì‹œíŠ¸ ìë™ ìƒì„±)

uploader.upload_data(df, worksheet_name=get_current_sheet_name())

# 2. ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ
# 3ê°œì›” ì´ìƒ ë°ì´í„°ëŠ” ë³„ë„ ì‹œíŠ¸ë¡œ ì´ë™
def archive_old_data(uploader, months=3):
    from datetime import datetime, timedelta
    cutoff_date = datetime.now() - timedelta(days=months * 30)

    current_sheet = uploader.spreadsheet.worksheet('í¬ë¡¤ë§ ê²°ê³¼')
    archive_sheet = uploader.spreadsheet.worksheet('ì•„ì¹´ì´ë¸Œ')

    all_data = current_sheet.get_all_values()
    old_data = [row for row in all_data if parse_date(row[4]) < cutoff_date]

    archive_sheet.append_rows(old_data)
    # í˜„ì¬ ì‹œíŠ¸ì—ì„œ ì‚­ì œ ë¡œì§...
```

**íŒì •**: **1ë…„ ë°ì´í„°(~50,000ê±´) ì´ˆê³¼ ì‹œ Supabase/Postgres ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìˆ˜**

---

### 4. í˜ì´ì§€ë„¤ì´ì…˜ ì „ëµ

#### âœ… **ì„œë²„ ì¸¡ í˜ì´ì§€ë„¤ì´ì…˜ + ê°€ìƒ ìŠ¤í¬ë¡¤**

**êµ¬í˜„**:

```typescript
// app/api/listings/route.ts
export async function GET(request: NextRequest) {
  const searchParams = request.nextUrl.searchParams;
  const page = parseInt(searchParams.get('page') || '1');
  const limit = parseInt(searchParams.get('limit') || '50');
  const sortBy = searchParams.get('sortBy') || 'ì‘ì„±ì¼';
  const sortOrder = searchParams.get('sortOrder') || 'desc';

  const allData = await getSheetData();
  const headers = allData[0];
  const rows = allData.slice(1);

  // ì •ë ¬
  const sortIndex = headers.indexOf(sortBy);
  const sorted = rows.sort((a, b) => {
    const aVal = a[sortIndex];
    const bVal = b[sortIndex];

    if (sortOrder === 'asc') {
      return aVal > bVal ? 1 : -1;
    } else {
      return aVal < bVal ? 1 : -1;
    }
  });

  // í˜ì´ì§€ë„¤ì´ì…˜
  const start = (page - 1) * limit;
  const end = start + limit;
  const paginated = sorted.slice(start, end);

  return Response.json({
    data: paginated,
    pagination: {
      page,
      limit,
      total: rows.length,
      totalPages: Math.ceil(rows.length / limit),
    },
  });
}

// components/PostsTable.tsx
'use client';
import { useInfiniteQuery } from '@tanstack/react-query';
import { useVirtualizer } from '@tanstack/react-virtual';

export function PostsTable() {
  const {
    data,
    fetchNextPage,
    hasNextPage,
    isFetchingNextPage,
  } = useInfiniteQuery({
    queryKey: ['listings'],
    queryFn: ({ pageParam = 1 }) =>
      fetch(`/api/listings?page=${pageParam}&limit=50`).then(r => r.json()),
    getNextPageParam: (lastPage) =>
      lastPage.pagination.page < lastPage.pagination.totalPages
        ? lastPage.pagination.page + 1
        : undefined,
  });

  const allRows = data?.pages.flatMap(page => page.data) ?? [];

  const parentRef = useRef<HTMLDivElement>(null);

  const virtualizer = useVirtualizer({
    count: allRows.length,
    getScrollElement: () => parentRef.current,
    estimateSize: () => 50, // í–‰ ë†’ì´ 50px
    overscan: 10,
  });

  return (
    <div ref={parentRef} style={{ height: '600px', overflow: 'auto' }}>
      <div style={{ height: `${virtualizer.getTotalSize()}px`, position: 'relative' }}>
        {virtualizer.getVirtualItems().map(virtualRow => (
          <div
            key={virtualRow.index}
            style={{
              position: 'absolute',
              top: 0,
              left: 0,
              width: '100%',
              height: `${virtualRow.size}px`,
              transform: `translateY(${virtualRow.start}px)`,
            }}
          >
            <PostRow data={allRows[virtualRow.index]} />
          </div>
        ))}
      </div>

      {hasNextPage && (
        <button onClick={() => fetchNextPage()} disabled={isFetchingNextPage}>
          {isFetchingNextPage ? 'ë¡œë”© ì¤‘...' : 'ë” ë³´ê¸°'}
        </button>
      )}
    </div>
  );
}
```

**íŒì •**: **Infinite Scroll + Virtual Scrolling ê¶Œì¥**

---

## ì—ëŸ¬ ì²˜ë¦¬

### 1. API ì¿¼í„° ì´ˆê³¼ ì‹œë‚˜ë¦¬ì˜¤

#### âš ï¸ **í˜„ì¬ ë¯¸êµ¬í˜„, ì¶”ê°€ í•„ìš”**

**ë¬¸ì œ ì½”ë“œ**:
```python
# upload_to_gsheet.py (í˜„ì¬)
try:
    worksheet.append_rows(values)
    print(f"[OK] ì—…ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"[ERROR] ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    return 0, 0
```

**ê°œì„ ì•ˆ**:

```python
from googleapiclient.errors import HttpError
import time

class GoogleSheetsUploader:
    def upload_data_with_retry(self, df, worksheet_name='í¬ë¡¤ë§ ê²°ê³¼', max_retries=3):
        """ì¬ì‹œë„ ë¡œì§ í¬í•¨ ì—…ë¡œë“œ"""
        for attempt in range(max_retries):
            try:
                return self.upload_data(df, worksheet_name)

            except HttpError as e:
                if e.resp.status == 429:  # Rate limit exceeded
                    retry_after = int(e.resp.get('Retry-After', 60))
                    print(f"[WARN] API ì¿¼í„° ì´ˆê³¼, {retry_after}ì´ˆ ëŒ€ê¸° (ì‹œë„ {attempt + 1}/{max_retries})")
                    time.sleep(retry_after)
                    continue

                elif e.resp.status == 403:  # Quota exceeded (ì¼ì¼ í•œë„)
                    print(f"[ERROR] ì¼ì¼ ì¿¼í„° ì´ˆê³¼, ë‚´ì¼ ë‹¤ì‹œ ì‹œë„")
                    raise

                elif e.resp.status in [500, 502, 503, 504]:  # Google ì„œë²„ ì˜¤ë¥˜
                    wait_time = 2 ** attempt  # Exponential backoff
                    print(f"[WARN] ì„œë²„ ì˜¤ë¥˜, {wait_time}ì´ˆ ëŒ€ê¸°")
                    time.sleep(wait_time)
                    continue

                else:
                    raise

            except Exception as e:
                print(f"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
                raise

        raise Exception(f"{max_retries}ë²ˆ ì¬ì‹œë„ í›„ ì‹¤íŒ¨")

# Next.js ì¸¡
// lib/googleSheets.ts
export async function getSheetDataWithRetry(range: string, maxRetries = 3) {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await getSheetData(range);
    } catch (error: any) {
      if (error.code === 429) {
        const retryAfter = error.response?.headers['retry-after'] || 60;
        console.warn(`Rate limit exceeded, waiting ${retryAfter}s`);
        await new Promise(resolve => setTimeout(resolve, retryAfter * 1000));
        continue;
      }

      if (error.code >= 500 && attempt < maxRetries - 1) {
        const waitTime = Math.pow(2, attempt) * 1000;
        await new Promise(resolve => setTimeout(resolve, waitTime));
        continue;
      }

      throw error;
    }
  }

  throw new Error(`Failed after ${maxRetries} retries`);
}
```

**íŒì •**: **ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ í•„ìˆ˜**

---

### 2. ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬

#### âš ï¸ **í˜„ì¬ ë¯¸êµ¬í˜„**

**ê°œì„ ì•ˆ**:

```python
# Python
from google.auth.transport.requests import Request
from google.auth.transport import requests as google_requests
import socket

# íƒ€ì„ì•„ì›ƒ ì„¤ì •
http = google_requests.AuthorizedSession(credentials)
http.timeout = 30  # 30ì´ˆ

# Next.js
// lib/googleSheets.ts
import { google } from 'googleapis';
import { Agent } from 'https';

const httpsAgent = new Agent({
  timeout: 30000, // 30ì´ˆ
  keepAlive: true,
});

const sheets = google.sheets({
  version: 'v4',
  auth,
  timeout: 30000,
  agent: httpsAgent,
});

// íƒ€ì„ì•„ì›ƒ ë˜í¼
export async function getSheetData(range: string, timeoutMs = 30000) {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

  try {
    const response = await sheets.spreadsheets.values.get({
      spreadsheetId: SPREADSHEET_ID,
      range,
    }, { signal: controller.signal });

    return response.data.values || [];
  } catch (error: any) {
    if (error.name === 'AbortError') {
      throw new Error('Google Sheets API request timed out');
    }
    throw error;
  } finally {
    clearTimeout(timeoutId);
  }
}
```

**íŒì •**: **íƒ€ì„ì•„ì›ƒ 30ì´ˆ ì„¤ì • ê¶Œì¥**

---

### 3. ì¸ì¦ ì‹¤íŒ¨ ë³µêµ¬

#### âš ï¸ **í˜„ì¬ ë‹¨ìˆœ ì—ëŸ¬ ì¶œë ¥ë§Œ**

**ê°œì„ ì•ˆ**:

```python
class GoogleSheetsUploader:
    def __init__(self, credentials_file, spreadsheet_id):
        self.credentials_file = credentials_file
        self.spreadsheet_id = spreadsheet_id
        self.client = None
        self.spreadsheet = None
        self._authenticated = False

    def authenticate(self, retry=True):
        """ì¬ì¸ì¦ ë¡œì§"""
        try:
            scopes = [...]
            credentials = Credentials.from_service_account_file(
                self.credentials_file,
                scopes=scopes
            )

            # í† í° ë§Œë£Œ í™•ì¸
            if credentials.expired:
                print("[INFO] í† í° ë§Œë£Œ, ê°±ì‹  ì¤‘...")
                credentials.refresh(Request())

            self.client = gspread.authorize(credentials)
            self.spreadsheet = self.client.open_by_key(self.spreadsheet_id)
            self._authenticated = True
            print("[OK] ì¸ì¦ ì„±ê³µ")
            return True

        except FileNotFoundError:
            print(f"[ERROR] ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.credentials_file}")
            return False

        except Exception as e:
            print(f"[ERROR] ì¸ì¦ ì‹¤íŒ¨: {e}")

            if retry and "Token has been expired" in str(e):
                print("[INFO] í† í° ë§Œë£Œë¡œ ì¬ì¸ì¦ ì‹œë„")
                return self.authenticate(retry=False)

            return False

    def _ensure_authenticated(self):
        """API í˜¸ì¶œ ì „ ì¸ì¦ ìƒíƒœ í™•ì¸"""
        if not self._authenticated:
            if not self.authenticate():
                raise Exception("ì¸ì¦ í•„ìš”")

# Next.js
// lib/googleSheets.ts
let authClient: JWT | null = null;

function getAuthClient() {
  if (!authClient) {
    authClient = new JWT({
      email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,
      key: process.env.GOOGLE_PRIVATE_KEY?.replace(/\\n/g, '\n'),
      scopes: SCOPES,
    });
  }

  return authClient;
}

export async function getSheetData(range: string) {
  try {
    const auth = getAuthClient();

    // í† í° ë§Œë£Œ í™•ì¸ ë° ìë™ ê°±ì‹ 
    const accessToken = await auth.getAccessToken();

    if (!accessToken.token) {
      throw new Error('Failed to get access token');
    }

    const sheets = google.sheets({ version: 'v4', auth });
    const response = await sheets.spreadsheets.values.get({
      spreadsheetId: SPREADSHEET_ID,
      range,
    });

    return response.data.values || [];
  } catch (error: any) {
    if (error.code === 401) {
      console.error('Authentication failed, refreshing credentials...');
      authClient = null;  // ì¸ì¦ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
      return getSheetData(range);  // ì¬ì‹œë„
    }

    throw error;
  }
}
```

**íŒì •**: **ìë™ í† í° ê°±ì‹  ë¡œì§ ì¶”ê°€ ê¶Œì¥**

---

### 4. Fallback ë©”ì»¤ë‹ˆì¦˜

#### âŒ **í˜„ì¬ ë¯¸êµ¬í˜„, ì¶”ê°€ ê°•ë ¥ ê¶Œì¥**

**êµ¬í˜„ì•ˆ**:

```typescript
// lib/googleSheets.ts
import fs from 'fs/promises';
import path from 'path';

const CACHE_FILE = path.join(process.cwd(), 'data', 'cache', 'sheet-data.json');
const CACHE_TTL = 3600 * 1000; // 1ì‹œê°„

export async function getSheetDataWithFallback(range: string) {
  try {
    // 1ì°¨ ì‹œë„: Google Sheets API
    const data = await getSheetDataWithRetry(range);

    // ì„±ê³µ ì‹œ ìºì‹œ ì €ì¥
    await saveCacheToFile(data);

    return data;
  } catch (error) {
    console.error('Google Sheets API failed, using fallback:', error);

    try {
      // 2ì°¨ ì‹œë„: ë¡œì»¬ ìºì‹œ íŒŒì¼
      const cachedData = await loadCacheFromFile();

      if (cachedData) {
        console.warn('Using cached data from file');
        return cachedData.data;
      }
    } catch (cacheError) {
      console.error('Cache fallback failed:', cacheError);
    }

    // 3ì°¨ ì‹œë„: ë¹ˆ ë°ì´í„° ë°˜í™˜ (ì•± í¬ë˜ì‹œ ë°©ì§€)
    console.error('All fallback methods failed, returning empty data');
    return [];
  }
}

async function saveCacheToFile(data: any[][]) {
  const cacheData = {
    data,
    timestamp: Date.now(),
  };

  await fs.mkdir(path.dirname(CACHE_FILE), { recursive: true });
  await fs.writeFile(CACHE_FILE, JSON.stringify(cacheData), 'utf-8');
}

async function loadCacheFromFile() {
  try {
    const fileContent = await fs.readFile(CACHE_FILE, 'utf-8');
    const cacheData = JSON.parse(fileContent);

    // ìºì‹œ ë§Œë£Œ í™•ì¸
    if (Date.now() - cacheData.timestamp > CACHE_TTL) {
      console.warn('Cache expired');
      return null;
    }

    return cacheData;
  } catch (error) {
    return null;
  }
}

// Python í¬ë¡¤ëŸ¬ë„ ë™ì¼í•˜ê²Œ ë¡œì»¬ ë°±ì—… ì €ì¥
def save_backup_csv(df):
    backup_dir = 'backups'
    os.makedirs(backup_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_file = f'{backup_dir}/backup_{timestamp}.csv'

    df.to_csv(backup_file, index=False, encoding='utf-8-sig')
    print(f"[INFO] ë°±ì—… ì €ì¥: {backup_file}")
```

**íŒì •**: **Fallback ìºì‹œ êµ¬í˜„ í•„ìˆ˜**

---

## ëŒ€ì•ˆ ê²€í† 

### 1. Google Sheets vs Supabase

#### ë¹„êµí‘œ

| í•­ëª© | Google Sheets | Supabase (PostgreSQL) | íŒì • |
|------|--------------|----------------------|------|
| **ë¹„ìš©** | ë¬´ë£Œ (API ì¿¼í„° ë‚´) | ë¬´ë£Œ (500MB DB, 2GB ì „ì†¡) | ğŸŸ° ë™ì  |
| **ì„¤ì • ë³µì¡ë„** | â­â­ ê°„ë‹¨ (Service Accountë§Œ) | â­â­â­ ë³´í†µ (DB ì„¤ì • í•„ìš”) | ğŸŸ¢ Sheets |
| **ì¿¼ë¦¬ ì„±ëŠ¥** | â­â­ ëŠë¦¼ (ì „ì²´ ìŠ¤ìº”) | â­â­â­â­â­ ë§¤ìš° ë¹ ë¦„ (ì¸ë±ìŠ¤) | ğŸ”´ Sheets |
| **ë°ì´í„° í™•ì¥ì„±** | â­â­ 10,000ê±´ í•œê³„ | â­â­â­â­â­ ë¬´ì œí•œ (ì‹¤ì§ˆì ) | ğŸ”´ Sheets |
| **ì‹¤ì‹œê°„ ë™ê¸°í™”** | â­â­ í´ë§ í•„ìš” | â­â­â­â­â­ Realtime Subscriptions | ğŸ”´ Sheets |
| **ì‚¬ìš©ì í˜‘ì—…** | â­â­â­â­â­ ì—‘ì…€ì²˜ëŸ¼ í¸ì§‘ | â­â­ SQL ì§€ì‹ í•„ìš” | ğŸŸ¢ Sheets |
| **ë°ì´í„° ë°±ì—…** | â­â­â­ Google Drive ìë™ | â­â­â­â­ pg_dump | ğŸŸ° ë™ì  |
| **API Rate Limit** | â­â­â­ 100 req/min | â­â­â­â­â­ ë¬´ì œí•œ (DB ì§ì ‘ ì ‘ê·¼) | ğŸ”´ Sheets |
| **íƒ€ì… ì•ˆì •ì„±** | â­ íƒ€ì… ê°•ì œ ì—†ìŒ | â­â­â­â­â­ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ | ğŸ”´ Sheets |
| **BI ë„êµ¬ ì—°ë™** | â­â­â­â­ Looker Studio ì§ì ‘ ì—°ë™ | â­â­â­â­â­ Metabase, Redash ë“± | ğŸŸ° ë™ì  |

**ì¢…í•© í‰ê°€**:
- **í˜„ì¬ ë‹¨ê³„ (MVP, 1000ê±´ ë¯¸ë§Œ)**: ğŸŸ¢ **Google Sheets ì í•©**
- **í™•ì¥ ë‹¨ê³„ (1000ê±´ ì´ìƒ, ë‹¤ìˆ˜ ì‚¬ìš©ì)**: ğŸ”´ **Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”**

---

### 2. Google Sheets vs Airtable

| í•­ëª© | Google Sheets | Airtable | íŒì • |
|------|--------------|----------|------|
| ë¹„ìš© (ë¬´ë£Œ í”Œëœ) | ë¬´ì œí•œ | 1,000 records/base | ğŸŸ¢ Sheets |
| API ì„±ëŠ¥ | â­â­â­ | â­â­â­â­ (REST API ìµœì í™”) | ğŸ”´ Sheets |
| UI/UX | â­â­â­ ì—‘ì…€ ìŠ¤íƒ€ì¼ | â­â­â­â­â­ ëª¨ë˜ UI | ğŸ”´ Sheets |
| í•„í„°/ì •ë ¬ | â­â­â­ ê¸°ë³¸ ê¸°ëŠ¥ | â­â­â­â­â­ ê°•ë ¥í•œ ë·° | ğŸ”´ Sheets |
| ìë™í™” | â­â­ Apps Script | â­â­â­â­ Automations | ğŸ”´ Sheets |
| í•œêµ­ì–´ ì§€ì› | â­â­â­â­â­ ì™„ë²½ | â­â­â­ ë¶€ë¶„ | ğŸŸ¢ Sheets |

**íŒì •**: **ë¬´ë£Œ í”Œëœì—ì„œëŠ” Google Sheets ìš°ìœ„**

---

### 3. CSV íŒŒì¼ ì§ì ‘ ì„œë¹™ (Vercel Blob Storage)

#### ë¹„êµ

**Google Sheets ë°©ì‹**:
```
Python í¬ë¡¤ëŸ¬ â†’ Google Sheets API â†’ Next.js â†’ ì‚¬ìš©ì
(ë„¤íŠ¸ì›Œí¬ ì™•ë³µ 2íšŒ)
```

**Vercel Blob ë°©ì‹**:
```
Python í¬ë¡¤ëŸ¬ â†’ CSV ì—…ë¡œë“œ â†’ Vercel Blob â†’ Next.js â†’ ì‚¬ìš©ì
(ë„¤íŠ¸ì›Œí¬ ì™•ë³µ 1íšŒ, CDN ìºì‹±)
```

**êµ¬í˜„ ì˜ˆì‹œ**:

```python
# Python í¬ë¡¤ëŸ¬
from vercel_blob import put

def upload_to_vercel_blob(df):
    csv_content = df.to_csv(index=False, encoding='utf-8')

    blob = put(
        pathname='marine-ministry-posts.csv',
        body=csv_content,
        access='public',
        token=os.getenv('BLOB_READ_WRITE_TOKEN')
    )

    print(f"[OK] Vercel Blob ì—…ë¡œë“œ: {blob['url']}")
    return blob['url']

# Next.js
// lib/fetchData.ts
export async function getPostsFromBlob() {
  const blobUrl = process.env.NEXT_PUBLIC_BLOB_URL;
  const response = await fetch(blobUrl, { next: { revalidate: 300 } });
  const csvText = await response.text();

  // CSV íŒŒì‹±
  const lines = csvText.split('\n');
  const headers = lines[0].split(',');
  const data = lines.slice(1).map(line => {
    const values = line.split(',');
    return headers.reduce((obj, header, index) => {
      obj[header] = values[index];
      return obj;
    }, {});
  });

  return data;
}
```

**ë¹„êµí‘œ**:

| í•­ëª© | Google Sheets | Vercel Blob (CSV) | íŒì • |
|------|--------------|-------------------|------|
| ë¹„ìš© | ë¬´ë£Œ | $0.15/GB ì €ì¥, $0.30/GB ì „ì†¡ | ğŸŸ¢ Sheets (ì†Œê·œëª¨) |
| ì„±ëŠ¥ | ~1ì´ˆ (API í˜¸ì¶œ) | ~0.1ì´ˆ (CDN) | ğŸ”´ Sheets |
| í˜‘ì—… í¸ì§‘ | â­â­â­â­â­ | âŒ ë¶ˆê°€ëŠ¥ | ğŸŸ¢ Sheets |
| ë²„ì „ ê´€ë¦¬ | â­â­ ìˆ˜ë™ | â­â­â­ Blob ë²„ì „ | ğŸ”´ Sheets |
| í•„í„°ë§ | í´ë¼ì´ì–¸íŠ¸ ì¸¡ | í´ë¼ì´ì–¸íŠ¸ ì¸¡ | ğŸŸ° ë™ì  |

**íŒì •**: **í˜‘ì—… í•„ìš” ì‹œ Sheets, ì„±ëŠ¥ ìš°ì„  ì‹œ Blob**

---

### 4. ì¢…í•© ì¶”ì²œ ë¡œë“œë§µ

#### Phase 1: MVP (í˜„ì¬ ~ 1,000ê±´)
```
âœ… Google Sheets ì‚¬ìš©
- ì„¤ì • ê°„ë‹¨, ë¬´ë£Œ
- ë¹„ê¸°ìˆ ìë„ ë°ì´í„° ìˆ˜ì • ê°€ëŠ¥
- Looker Studio ì—°ë™ ìš©ì´
```

#### Phase 2: í™•ì¥ (1,000 ~ 10,000ê±´)
```
ğŸ”„ Supabase PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜
- ë¬´ë£Œ í”Œëœ 500MB (ì¶©ë¶„)
- Row Level Security (ì‚¬ìš©ìë³„ ê¶Œí•œ)
- Realtime Subscriptions (ì‹¤ì‹œê°„ ë™ê¸°í™”)
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ì½”ë“œ**:
```python
# Python í¬ë¡¤ëŸ¬
from supabase import create_client

supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_KEY')
)

def upload_to_supabase(df):
    # DataFrame â†’ dict list ë³€í™˜
    records = df.to_dict('records')

    # Upsert (ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸)
    response = supabase.table('marine_posts').upsert(
        records,
        on_conflict='link',  # ë§í¬ ê¸°ì¤€ ì¤‘ë³µ í™•ì¸
        returning='minimal'
    ).execute()

    print(f"[OK] Supabase ì—…ë¡œë“œ: {len(records)}ê±´")

# Next.js
// lib/supabase.ts
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

export async function getPosts({ agency, board, startDate, endDate, page = 1, limit = 50 }) {
  let query = supabase
    .from('marine_posts')
    .select('*', { count: 'exact' });

  if (agency) query = query.eq('agency_name', agency);
  if (board) query = query.eq('board_type', board);
  if (startDate) query = query.gte('published_at', startDate);
  if (endDate) query = query.lte('published_at', endDate);

  const { data, error, count } = await query
    .order('published_at', { ascending: false })
    .range((page - 1) * limit, page * limit - 1);

  return { data, total: count };
}
```

#### Phase 3: ê¸€ë¡œë²Œ í™•ì¥ (10,000ê±´+)
```
ğŸš€ Vercel Postgres + Edge Functions
- ê¸€ë¡œë²Œ CDN
- Edge Runtime (ì§€ì—°ì‹œê°„ < 100ms)
- Serverless Postgres
```

---

## ê¶Œì¥ì‚¬í•­ ë° ê²°ë¡ 

### âœ… ì¦‰ì‹œ ì ìš© (Phase 1 ê°œì„ )

#### 1. Python í¬ë¡¤ëŸ¬
```python
# âœ… í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
import os
CREDENTIALS_FILE = os.getenv('GOOGLE_CREDENTIALS_PATH')
SPREADSHEET_ID = os.getenv('GOOGLE_SPREADSHEET_ID')

# âœ… ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
uploader.upload_data_with_retry(df, max_retries=3)

# âœ… URL ì •ê·œí™”
from urllib.parse import urlparse

def normalize_url(url):
    parsed = urlparse(url.lower())
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

df['ë§í¬_ì •ê·œí™”'] = df['ë§í¬'].apply(normalize_url)

# âœ… ë¡œì»¬ ë°±ì—…
df.to_csv(f'backups/backup_{datetime.now():%Y%m%d_%H%M%S}.csv')
```

#### 2. Next.js ëŒ€ì‹œë³´ë“œ
```typescript
// âœ… googleapis ì‚¬ìš©
import { google } from 'googleapis';

// âœ… ì½ê¸° ì „ìš© Service Account ìƒì„±
const SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly'];

// âœ… ISR 5ë¶„ + On-Demand Revalidation
export const revalidate = 300;

// app/api/revalidate/route.ts
export async function POST(request: Request) {
  revalidatePath('/dashboard');
  return Response.json({ revalidated: true });
}

// âœ… Fallback ìºì‹œ
const cachedData = await getSheetDataWithFallback();
```

#### 3. ì¸í”„ë¼
```bash
# âœ… í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Vercel)
vercel env add GOOGLE_SERVICE_ACCOUNT_EMAIL_READONLY
vercel env add GOOGLE_PRIVATE_KEY_READONLY
vercel env add GOOGLE_SPREADSHEET_ID
vercel env add REVALIDATE_SECRET

# âœ… Python í¬ë¡¤ëŸ¬ í™˜ê²½ë³€ìˆ˜
export GOOGLE_CREDENTIALS_PATH=/path/to/credentials.json
export GOOGLE_SPREADSHEET_ID=1lXwc_...

# âœ… í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ë§ (cron)
0 */6 * * * /usr/bin/python3 /path/to/marine_ministry_crawler.py
```

---

### ğŸ”„ ì¤‘ê¸° ê³„íš (3-6ê°œì›”, 1000ê±´ ì´ìƒ ì‹œ)

#### Supabase ë§ˆì´ê·¸ë ˆì´ì…˜
```sql
-- Supabase í…Œì´ë¸” ìƒì„±
CREATE TABLE marine_posts (
  id BIGSERIAL PRIMARY KEY,
  agency_category TEXT NOT NULL,
  agency_name TEXT NOT NULL,
  board_type TEXT NOT NULL,
  title TEXT NOT NULL,
  published_at DATE NOT NULL,
  link TEXT UNIQUE NOT NULL,
  collected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

  -- ì¸ë±ìŠ¤
  CONSTRAINT unique_link UNIQUE(link)
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_agency_name ON marine_posts(agency_name);
CREATE INDEX idx_board_type ON marine_posts(board_type);
CREATE INDEX idx_published_at ON marine_posts(published_at DESC);
CREATE INDEX idx_collected_at ON marine_posts(collected_at DESC);

-- ë³µí•© ì¸ë±ìŠ¤
CREATE INDEX idx_agency_board ON marine_posts(agency_name, board_type);

-- Full-text search (í•œê¸€ ì§€ì›)
ALTER TABLE marine_posts ADD COLUMN title_search TSVECTOR;
CREATE INDEX idx_title_search ON marine_posts USING GIN(title_search);
```

---

### ğŸ“Š ìµœì¢… ê¶Œì¥ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     í•´ì–‘ë¶€ì²˜ í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1 (í˜„ì¬ ~ 1,000ê±´):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python       â”‚â”€â”€â”€â–¶â”‚ Google       â”‚â—€â”€â”€â”€â”‚ Next.js      â”‚
â”‚ Crawler      â”‚    â”‚ Sheets       â”‚    â”‚ Dashboard    â”‚
â”‚              â”‚    â”‚ (ë¬´ë£Œ)        â”‚    â”‚ (Vercel)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Looker       â”‚
                    â”‚ Studio       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 2 (1,000 ~ 10,000ê±´):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python       â”‚â”€â”€â”€â–¶â”‚ Supabase     â”‚â—€â”€â”€â”€â”‚ Next.js      â”‚
â”‚ Crawler      â”‚    â”‚ PostgreSQL   â”‚    â”‚ Dashboard    â”‚
â”‚              â”‚    â”‚ (ë¬´ë£Œ 500MB)  â”‚    â”‚ (Vercel)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Metabase     â”‚
                    â”‚ (BI)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 3 (10,000ê±´+):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python       â”‚â”€â”€â”€â–¶â”‚ Vercel       â”‚â—€â”€â”€â”€â”‚ Next.js      â”‚
â”‚ Crawler      â”‚    â”‚ Postgres     â”‚    â”‚ Edge Runtime â”‚
â”‚ (Cloud Run)  â”‚    â”‚ (Serverless) â”‚    â”‚ (ê¸€ë¡œë²Œ CDN) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ¯ í•µì‹¬ ê²°ë¡ 

#### âœ… Google Sheets ì‚¬ìš© ê°€ëŠ¥ ì¡°ê±´
1. **ë°ì´í„° ê·œëª¨**: 1,000ê±´ ë¯¸ë§Œ
2. **ì‚¬ìš©ì ìˆ˜**: ë™ì‹œ ì ‘ì† 50ëª… ì´í•˜
3. **ì—…ë°ì´íŠ¸ ë¹ˆë„**: 1ì‹œê°„ 1íšŒ ì´í•˜
4. **í˜‘ì—… í•„ìš”**: ë¹„ê¸°ìˆ ìê°€ ë°ì´í„° ìˆ˜ì •

#### âŒ Google Sheets í•œê³„ ë„ë‹¬ ì‹œê·¸ë„
1. **API ì‘ë‹µ ì‹œê°„** > 3ì´ˆ
2. **API Rate Limit ì—ëŸ¬** ì£¼ 1íšŒ ì´ìƒ ë°œìƒ
3. **ë°ì´í„° ê±´ìˆ˜** > 5,000ê±´
4. **ì‚¬ìš©ì ë¶ˆë§Œ** (ëŠë¦° ë¡œë”©, íƒ€ì„ì•„ì›ƒ)

#### ğŸš€ í˜„ì¬ í”„ë¡œì íŠ¸ íŒì •
- **Python í¬ë¡¤ëŸ¬**: ğŸŸ¢ **í”„ë¡œë•ì…˜ ë ˆë””**
- **Next.js ì„¤ê³„**: ğŸŸ¡ **ISR + Fallback ì¶”ê°€ í•„ìš”**
- **í™•ì¥ì„±**: ğŸŸ¡ **1ë…„ ë‚´ Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ ê¶Œì¥**

---

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì¦‰ì‹œ êµ¬í˜„ (1ì£¼)
- [ ] Python í¬ë¡¤ëŸ¬ í™˜ê²½ë³€ìˆ˜ ì ìš©
- [ ] ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (exponential backoff)
- [ ] URL ì •ê·œí™” í•¨ìˆ˜ ì¶”ê°€
- [ ] ë¡œì»¬ ë°±ì—… CSV ì €ì¥

#### Next.js êµ¬í˜„ (2ì£¼)
- [ ] googleapis ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
- [ ] ì½ê¸° ì „ìš© Service Account ìƒì„±
- [ ] ISR 5ë¶„ ìºì‹± ì ìš©
- [ ] On-Demand Revalidation API êµ¬í˜„
- [ ] Fallback ìºì‹œ ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
- [ ] ì—ëŸ¬ ë°”ìš´ë”ë¦¬ êµ¬í˜„

#### ì¸í”„ë¼ (1ì£¼)
- [ ] Vercel í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [ ] Python í¬ë¡¤ëŸ¬ cron ìŠ¤ì¼€ì¤„ë§
- [ ] Google Cloud Monitoring ì„¤ì •
- [ ] ë°±ì—… ìë™í™” ìŠ¤í¬ë¦½íŠ¸

#### ëª¨ë‹ˆí„°ë§ (ì§€ì†)
- [ ] API ì¿¼í„° ì‚¬ìš©ëŸ‰ ëŒ€ì‹œë³´ë“œ
- [ ] í‰ê·  ì‘ë‹µ ì‹œê°„ ì¶”ì 
- [ ] ì—ëŸ¬ ë¡œê·¸ ìˆ˜ì§‘ (Sentry)
- [ ] ì›”ë³„ ë°ì´í„° ì¦ê°€ëŸ‰ ë¶„ì„

---

## ë¶€ë¡: ì½”ë“œ ìŠ¤ë‹ˆí«

### A. ì™„ì „í•œ Next.js êµ¬í˜„

```typescript
// lib/googleSheets.ts
import { google } from 'googleapis';
import { JWT } from 'google-auth-library';
import fs from 'fs/promises';
import path from 'path';

const SPREADSHEET_ID = process.env.GOOGLE_SPREADSHEET_ID!;
const CACHE_DIR = path.join(process.cwd(), '.cache');
const CACHE_FILE = path.join(CACHE_DIR, 'sheet-data.json');

let authClient: JWT | null = null;

function getAuthClient() {
  if (!authClient) {
    authClient = new JWT({
      email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,
      key: process.env.GOOGLE_PRIVATE_KEY?.replace(/\\n/g, '\n'),
      scopes: ['https://www.googleapis.com/auth/spreadsheets.readonly'],
    });
  }
  return authClient;
}

export async function getSheetData(range: string = 'í¬ë¡¤ë§ ê²°ê³¼!A:G') {
  const auth = getAuthClient();
  const sheets = google.sheets({ version: 'v4', auth });

  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 30000);

  try {
    const response = await sheets.spreadsheets.values.get({
      spreadsheetId: SPREADSHEET_ID,
      range,
    }, { signal: controller.signal });

    const data = response.data.values || [];

    // ìºì‹œ ì €ì¥
    await saveCache(data);

    return data;
  } catch (error: any) {
    console.error('Google Sheets API Error:', error);

    if (error.code === 401) {
      authClient = null;
      return getSheetData(range);
    }

    // Fallback to cache
    const cached = await loadCache();
    if (cached) {
      console.warn('Using cached data due to API error');
      return cached;
    }

    throw error;
  } finally {
    clearTimeout(timeoutId);
  }
}

async function saveCache(data: any[][]) {
  try {
    await fs.mkdir(CACHE_DIR, { recursive: true });
    await fs.writeFile(
      CACHE_FILE,
      JSON.stringify({ data, timestamp: Date.now() }),
      'utf-8'
    );
  } catch (error) {
    console.error('Failed to save cache:', error);
  }
}

async function loadCache() {
  try {
    const content = await fs.readFile(CACHE_FILE, 'utf-8');
    const { data, timestamp } = JSON.parse(content);

    // 1ì‹œê°„ ì´ë‚´ ìºì‹œë§Œ ì‚¬ìš©
    if (Date.now() - timestamp < 3600000) {
      return data;
    }
  } catch (error) {
    console.error('Failed to load cache:', error);
  }
  return null;
}

// app/dashboard/page.tsx
import { getSheetData } from '@/lib/googleSheets';
import { StatCards } from '@/components/StatCards';
import { RecentPostsTable } from '@/components/RecentPostsTable';

export const revalidate = 300; // 5ë¶„ ISR

export default async function DashboardPage() {
  const data = await getSheetData();
  const headers = data[0];
  const rows = data.slice(1);

  // í†µê³„ ê³„ì‚°
  const totalPosts = rows.length;
  const agencies = new Set(rows.map(row => row[1])).size;
  const today = new Date().toISOString().split('T')[0];
  const todayPosts = rows.filter(row => row[4] === today).length;

  return (
    <div className="container mx-auto p-6">
      <h1 className="text-3xl font-bold mb-6">í•´ì–‘ë¶€ì²˜ ëŒ€ì‹œë³´ë“œ</h1>

      <StatCards
        totalPosts={totalPosts}
        agencies={agencies}
        todayPosts={todayPosts}
      />

      <RecentPostsTable data={rows.slice(0, 10)} headers={headers} />
    </div>
  );
}

// app/api/revalidate/route.ts
import { revalidatePath } from 'next/cache';
import { NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const secret = request.headers.get('x-revalidate-secret');

  if (secret !== process.env.REVALIDATE_SECRET) {
    return Response.json({ error: 'Invalid secret' }, { status: 401 });
  }

  revalidatePath('/dashboard');

  return Response.json({
    revalidated: true,
    timestamp: new Date().toISOString(),
  });
}
```

### B. ê°œì„ ëœ Python í¬ë¡¤ëŸ¬

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ëª¨ë“ˆ (ê°œì„  ë²„ì „)
"""

import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.errors import HttpError
import pandas as pd
from datetime import datetime
import pytz
import os
import time
from urllib.parse import urlparse
import requests

class GoogleSheetsUploaderV2:
    """êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë” (ê°œì„  ë²„ì „)"""

    def __init__(self):
        self.credentials_file = os.getenv('GOOGLE_CREDENTIALS_PATH')
        self.spreadsheet_id = os.getenv('GOOGLE_SPREADSHEET_ID')
        self.revalidate_url = os.getenv('NEXTJS_REVALIDATE_URL')
        self.revalidate_secret = os.getenv('REVALIDATE_SECRET')

        if not all([self.credentials_file, self.spreadsheet_id]):
            raise ValueError("í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”: GOOGLE_CREDENTIALS_PATH, GOOGLE_SPREADSHEET_ID")

        self.client = None
        self.spreadsheet = None
        self._authenticated = False

    def normalize_url(self, url):
        """URL ì •ê·œí™”"""
        try:
            parsed = urlparse(url.lower())
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        except:
            return url.lower()

    def authenticate(self, retry=True):
        """êµ¬ê¸€ ì‹œíŠ¸ ì¸ì¦ (ì¬ì‹œë„ í¬í•¨)"""
        try:
            scopes = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]

            credentials = Credentials.from_service_account_file(
                self.credentials_file,
                scopes=scopes
            )

            if credentials.expired:
                print("[INFO] í† í° ë§Œë£Œ, ê°±ì‹  ì¤‘...")
                credentials.refresh(Request())

            self.client = gspread.authorize(credentials)
            self.spreadsheet = self.client.open_by_key(self.spreadsheet_id)
            self._authenticated = True

            print(f"[OK] êµ¬ê¸€ ì‹œíŠ¸ ì¸ì¦ ì„±ê³µ")
            return True

        except FileNotFoundError:
            print(f"[ERROR] ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.credentials_file}")
            return False

        except Exception as e:
            print(f"[ERROR] êµ¬ê¸€ ì‹œíŠ¸ ì¸ì¦ ì‹¤íŒ¨: {e}")

            if retry and "Token has been expired" in str(e):
                print("[INFO] ì¬ì¸ì¦ ì‹œë„")
                return self.authenticate(retry=False)

            return False

    def upload_data_with_retry(self, df, worksheet_name='í¬ë¡¤ë§ ê²°ê³¼', max_retries=3):
        """ì¬ì‹œë„ ë¡œì§ í¬í•¨ ì—…ë¡œë“œ"""
        for attempt in range(max_retries):
            try:
                # URL ì •ê·œí™”
                df['ë§í¬_ì •ê·œí™”'] = df['ë§í¬'].apply(self.normalize_url)

                # ë¡œì»¬ ë°±ì—…
                self.save_backup(df)

                # ì—…ë¡œë“œ
                added, duplicated = self.upload_data(df, worksheet_name)

                # Next.js ìºì‹œ ê°±ì‹ 
                if added > 0:
                    self.trigger_nextjs_revalidation()

                return added, duplicated

            except HttpError as e:
                if e.resp.status == 429:
                    retry_after = int(e.resp.get('Retry-After', 60))
                    print(f"[WARN] API ì¿¼í„° ì´ˆê³¼, {retry_after}ì´ˆ ëŒ€ê¸° (ì‹œë„ {attempt + 1}/{max_retries})")
                    time.sleep(retry_after)
                    continue

                elif e.resp.status == 403:
                    print(f"[ERROR] ì¼ì¼ ì¿¼í„° ì´ˆê³¼")
                    raise

                elif e.resp.status in [500, 502, 503, 504]:
                    wait_time = 2 ** attempt
                    print(f"[WARN] ì„œë²„ ì˜¤ë¥˜, {wait_time}ì´ˆ ëŒ€ê¸°")
                    time.sleep(wait_time)
                    continue

                else:
                    raise

            except Exception as e:
                print(f"[ERROR] ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
                raise

        raise Exception(f"{max_retries}ë²ˆ ì¬ì‹œë„ í›„ ì‹¤íŒ¨")

    def upload_data(self, df, worksheet_name='í¬ë¡¤ë§ ê²°ê³¼'):
        """ë°ì´í„° ì—…ë¡œë“œ (ê¸°ì¡´ ë¡œì§)"""
        try:
            worksheet = self.spreadsheet.worksheet(worksheet_name)
        except gspread.exceptions.WorksheetNotFound:
            worksheet = self.spreadsheet.add_worksheet(
                title=worksheet_name,
                rows=1000,
                cols=10
            )
            headers = list(df.columns)
            worksheet.append_row(headers)

        existing_df = self.get_existing_data(worksheet_name)

        seoul_tz = pytz.timezone('Asia/Seoul')
        current_time = datetime.now(seoul_tz).strftime('%Y-%m-%d %H:%M:%S')
        df['ìˆ˜ì§‘ì¼ì‹œ'] = current_time

        if not existing_df.empty and 'ë§í¬_ì •ê·œí™”' in existing_df.columns:
            existing_links = set(existing_df['ë§í¬_ì •ê·œí™”'].tolist())
            new_df = df[~df['ë§í¬_ì •ê·œí™”'].isin(existing_links)].copy()
            duplicate_count = len(df) - len(new_df)
        else:
            new_df = df.copy()
            duplicate_count = 0

        if new_df.empty:
            print(f"[INFO] ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0, duplicate_count

        # ì •ê·œí™” ì»¬ëŸ¼ ì œê±° (ì‹œíŠ¸ì— ì €ì¥ ì•ˆ í•¨)
        new_df = new_df.drop(columns=['ë§í¬_ì •ê·œí™”'])

        values = new_df.values.tolist()
        worksheet.append_rows(values)

        print(f"[OK] êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ: {len(new_df)}ê±´")
        return len(new_df), duplicate_count

    def save_backup(self, df):
        """ë¡œì»¬ ë°±ì—… ì €ì¥"""
        backup_dir = 'backups'
        os.makedirs(backup_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f'{backup_dir}/backup_{timestamp}.csv'

        # ì •ê·œí™” ì»¬ëŸ¼ ì œê±°
        df_backup = df.drop(columns=['ë§í¬_ì •ê·œí™”'], errors='ignore')
        df_backup.to_csv(backup_file, index=False, encoding='utf-8-sig')

        print(f"[INFO] ë°±ì—… ì €ì¥: {backup_file}")

    def trigger_nextjs_revalidation(self):
        """Next.js ìºì‹œ ê°±ì‹  íŠ¸ë¦¬ê±°"""
        if not self.revalidate_url or not self.revalidate_secret:
            print("[WARN] Next.js revalidation ì„¤ì • ì—†ìŒ")
            return

        try:
            response = requests.post(
                self.revalidate_url,
                headers={'x-revalidate-secret': self.revalidate_secret},
                timeout=10
            )

            if response.status_code == 200:
                print("[OK] Next.js ìºì‹œ ê°±ì‹  ì™„ë£Œ")
            else:
                print(f"[WARN] Next.js ìºì‹œ ê°±ì‹  ì‹¤íŒ¨: {response.status_code}")

        except Exception as e:
            print(f"[WARN] Next.js ìºì‹œ ê°±ì‹  ì˜¤ë¥˜: {e}")

    def get_existing_data(self, worksheet_name='í¬ë¡¤ë§ ê²°ê³¼'):
        """ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œì§)"""
        # ... (ê¸°ì¡´ ì½”ë“œ ë™ì¼)
        pass


def main():
    """í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    uploader = GoogleSheetsUploaderV2()

    if not uploader.authenticate():
        return

    csv_file = 'marine_ministry_posts_20251118.csv'
    df = pd.read_csv(csv_file, encoding='utf-8-sig')

    added, duplicated = uploader.upload_data_with_retry(df, max_retries=3)

    print(f"\n{'='*60}")
    print(f"ì—…ë¡œë“œ ì™„ë£Œ!")
    print(f"ìƒˆë¡œ ì¶”ê°€: {added}ê±´")
    print(f"ì¤‘ë³µ ì œì™¸: {duplicated}ê±´")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
```

---

## ë§ˆë¬´ë¦¬

ì´ ê¸°ìˆ  ê²€í†  ë¦¬í¬íŠ¸ëŠ” ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤:

1. âœ… **Python í¬ë¡¤ëŸ¬ ê²€ì¦**: í”„ë¡œë•ì…˜ ë ˆë””, ë§ˆì´ë„ˆ ê°œì„  ê¶Œì¥
2. âœ… **Next.js ì„¤ê³„ ê°€ì´ë“œ**: googleapis + ISR + Fallback ìºì‹±
3. âš ï¸ **ë¦¬ìŠ¤í¬ ë¶„ì„**: API ì¿¼í„°, ëŒ€ìš©ëŸ‰ ë°ì´í„° í•œê³„
4. ğŸ’¡ **ëŒ€ì•ˆ ì œì‹œ**: Supabase, Vercel Blob ë¹„êµ
5. ğŸ¯ **ëª…í™•í•œ ë¡œë“œë§µ**: Phase 1 (Sheets) â†’ Phase 2 (Supabase) â†’ Phase 3 (Vercel Postgres)

**ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ ì½”ë“œ ìŠ¤ë‹ˆí«ê³¼ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì—¬, ì´ ë¦¬í¬íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°”ë¡œ êµ¬í˜„ì— ì°©ìˆ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
